{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Library"
      ],
      "metadata": {
        "id": "0g4hsUZmE6tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Optional: Enable CUDA synchronous error reporting for debugging\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "9c73EAbiE8mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset & Normalize"
      ],
      "metadata": {
        "id": "b53ASiQzE--T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = '/home/abdul_desktop/jupyterENV/notebooks/CICIDS2017_dataset/cicids2017.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Inspect columns to identify the correct label column\n",
        "print(\"Columns in the dataset:\", data.columns)\n",
        "\n",
        "# Use the correct label column (adjust based on your dataset)\n",
        "label_column = ' Label' if ' Label' in data.columns else 'label'\n",
        "labels = data[label_column].astype(str).str.strip()  # Ensure labels are strings without spaces\n",
        "\n",
        "# Extract features by dropping the label column\n",
        "features = data.drop(columns=[label_column])\n",
        "\n",
        "# Handle missing values\n",
        "features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "features.fillna(features.mean(), inplace=True)\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Verify label encoding\n",
        "print(\"Unique labels after encoding:\", np.unique(labels_encoded))\n",
        "\n",
        "# Reshape data for Transformer (sequence_length)\n",
        "sequence_length = 10\n",
        "num_samples = len(features_scaled) - sequence_length\n",
        "X = np.array([features_scaled[i:i + sequence_length] for i in range(num_samples)])\n",
        "y = labels_encoded[sequence_length:]\n",
        "\n",
        "# Verify that labels are within the expected range\n",
        "print(\"Unique labels in y:\", np.unique(y))\n",
        "\n",
        "# Determine the number of classes\n",
        "num_classes = len(np.unique(y))\n",
        "print(f\"Number of classes: {num_classes}\")"
      ],
      "metadata": {
        "id": "uvMNvgJcFH7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split"
      ],
      "metadata": {
        "id": "QIVpni2JFPMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "print(f\"Train shape: {X_train_split.shape}, Validation shape: {X_val.shape}, Test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "FhWLc6USFOt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert & DataLoaders"
      ],
      "metadata": {
        "id": "OekNQpkjFXdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_split, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_split, dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "EikA6OGsFURc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check GPU"
      ],
      "metadata": {
        "id": "_2EbcxwYFcl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "QxNWU_x4FfG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Model Transformer & Custom"
      ],
      "metadata": {
        "id": "82Ije6LIFgFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Custom MultiheadAttention Layer\n",
        "class CustomMultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super(CustomMultiheadAttention, self).__init__()\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        attn_output, _ = self.multihead_attn(query, key, value)\n",
        "        attn_output = torch.relu(attn_output)\n",
        "        return attn_output\n",
        "\n",
        "# Custom Transformer Encoder Layer\n",
        "class CustomTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super(CustomTransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = CustomMultiheadAttention(embed_dim, num_heads, dropout)\n",
        "        self.linear1 = nn.Linear(embed_dim, embed_dim * 4)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(embed_dim * 4, embed_dim)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Self-attention part\n",
        "        src2 = self.self_attn(src, src, src)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Feedforward network\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "# Custom Transformer Model\n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, num_layers, output_dim, dropout=0.1):\n",
        "        super(CustomTransformerModel, self).__init__()\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [CustomTransformerEncoderLayer(input_dim, num_heads, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, feature_dim)\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, feature_dim)\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x)\n",
        "        x = x.mean(dim=0)  # (batch_size, feature_dim)\n",
        "        x = self.fc(x)     # (batch_size, output_dim)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cAkaWKrvFigS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper Param"
      ],
      "metadata": {
        "id": "aJy6fyGVFrSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_dim = X_train_tensor.shape[2]  # Number of features\n",
        "num_heads = 2                        # Number of attention heads\n",
        "num_layers = 2                       # Number of transformer layers\n",
        "output_dim = num_classes             # Number of classes\n",
        "dropout = 0.1\n",
        "num_epochs = 50                      # You can increase this as needed\n",
        "\n",
        "print(f\"Input Dimension: {input_dim}, Output Dimension: {output_dim}\")\n",
        "\n",
        "# Initialize the model, criterion, optimizer, and scheduler\n",
        "model = CustomTransformerModel(input_dim=input_dim, num_heads=num_heads, num_layers=num_layers, output_dim=output_dim, dropout=dropout).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Early stopping parameters\n",
        "early_stopping_patience = 5\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "# Lists to store loss and accuracy\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []"
      ],
      "metadata": {
        "id": "mEbjsIz4Fws1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train & Val"
      ],
      "metadata": {
        "id": "RqN0QV0FFy3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and validation loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss, correct_predictions, total_samples = 0.0, 0, 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "    train_loss = running_loss / total_samples\n",
        "    train_accuracy = correct_predictions / total_samples\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_running_loss, val_correct_predictions, val_total_samples = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct_predictions += (predicted == targets).sum().item()\n",
        "            val_total_samples += targets.size(0)\n",
        "\n",
        "    val_loss = val_running_loss / val_total_samples\n",
        "    val_accuracy = val_correct_predictions / val_total_samples\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print metrics for each epoch\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} - '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} - '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Step learning rate scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        # Optionally, save the best model\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break"
      ],
      "metadata": {
        "id": "kkUtcdD6F4oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate & Plotting"
      ],
      "metadata": {
        "id": "T3NWR2TUF-k5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX-I9iozE4kr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the best model (optional)\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "test_correct_predictions, test_total_samples = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct_predictions += (predicted == targets).sum().item()\n",
        "        test_total_samples += targets.size(0)\n",
        "\n",
        "test_accuracy = test_correct_predictions / test_total_samples\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}